{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jacob/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jacob/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jacob/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jacob/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_features = './data/stanfordSentimentTreebank/datasetSentences.txt'\n",
    "path_to_labels = './data/stanfordSentimentTreebank/sentiment_labels.txt'\n",
    "\n",
    "features_df = pd.read_csv(path_to_features, sep='\\t')\n",
    "labels_df = pd.read_csv(path_to_labels, sep='|', index_col=0)\n",
    "features_df.drop('sentence_index', axis=1, inplace=True)\n",
    "\n",
    "first_1000_f = features_df[:1000]\n",
    "first_1000_l = labels_df[:1000]\n",
    "\n",
    "data_df = pd.concat([first_1000_f, first_1000_l], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>0.44444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>0.42708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  sentiment values\n",
       "0  The Rock is destined to be the 21st Century 's...           0.50000\n",
       "1  The gorgeously elaborate continuation of `` Th...           0.50000\n",
       "2                     Effective but too-tepid biopic           0.44444\n",
       "3  If you sometimes like to go to the movies to h...           0.50000\n",
       "4  Emerges as something rare , an issue movie tha...           0.42708"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def tokenize_and_preprocess(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = [w for w in tokens if w.lower() not in stop_words and w not in string.punctuation]\n",
    "    nltk_tagged = nltk.pos_tag(tokens)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_tokens.append(word)\n",
    "        else:        \n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg_vector(sentence, model):\n",
    "    tokens = tokenize_and_preprocess(sentence)\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vector += model.wv[token]\n",
    "    if len(tokens) > 0:\n",
    "        vector /= len(tokens)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = data_df['sentence'].apply(tokenize_and_preprocess).tolist()\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=tokens_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "data_df['sentence_vector'] = data_df['sentence'].apply(lambda x: sentence_to_avg_vector(x, word2vec_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment values</th>\n",
       "      <th>sentence_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>[-0.0011338543486503609, 0.0006192612848032943...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>[0.0014520163885722666, 0.0011679331015013463,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>0.44444</td>\n",
       "      <td>[0.0061636255122721195, 0.0019347511309509475,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>[2.7271192973583107e-05, 0.002349162603624993,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>0.42708</td>\n",
       "      <td>[-0.00048030953845367406, 0.000448660732497676...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  sentiment values  \\\n",
       "0  The Rock is destined to be the 21st Century 's...           0.50000   \n",
       "1  The gorgeously elaborate continuation of `` Th...           0.50000   \n",
       "2                     Effective but too-tepid biopic           0.44444   \n",
       "3  If you sometimes like to go to the movies to h...           0.50000   \n",
       "4  Emerges as something rare , an issue movie tha...           0.42708   \n",
       "\n",
       "                                     sentence_vector  \n",
       "0  [-0.0011338543486503609, 0.0006192612848032943...  \n",
       "1  [0.0014520163885722666, 0.0011679331015013463,...  \n",
       "2  [0.0061636255122721195, 0.0019347511309509475,...  \n",
       "3  [2.7271192973583107e-05, 0.002349162603624993,...  \n",
       "4  [-0.00048030953845367406, 0.000448660732497676...  "
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, sentences_train, sentences_test = train_test_split(\n",
    "    data_df['sentence_vector'].tolist(), \n",
    "    data_df['sentiment values'].tolist(), \n",
    "    data_df['sentence'].tolist(),\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(100, 50) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.17938360571861267\n",
      "Epoch 101, Loss: 0.02870471030473709\n",
      "Epoch 201, Loss: 0.028493061661720276\n",
      "Epoch 301, Loss: 0.02828666940331459\n",
      "Epoch 401, Loss: 0.028082385659217834\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(np.vstack(X_train), dtype=torch.float)\n",
    "y = torch.tensor(y_train, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "# Instantiate the model, define loss function and optimizer\n",
    "model = SentimentNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 500\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor(np.vstack(X_test), dtype=torch.float)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.024493910372257233\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predictions = model(X_test)\n",
    "    test_loss = criterion(predictions, y_test) \n",
    "\n",
    "print(f'Test Loss: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: One scarcely needs the subtitles to enjoy this colorful action farce .\n",
      "Predicted Sentiment Value: 0.5326350927352905\n",
      "Actual Sentiment Value: 0.5138900279998779\n"
     ]
    }
   ],
   "source": [
    "index_to_check = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predictions = model(X_test)\n",
    "\n",
    "specific_test_example = X_test[index_to_check]\n",
    "predicted_value = predictions[index_to_check]\n",
    "actual_value = y_test[index_to_check]\n",
    "\n",
    "print(\"Original Sentence:\", sentences_test[index_to_check])\n",
    "print(\"Predicted Sentiment Value:\", predicted_value.item()) \n",
    "print(\"Actual Sentiment Value:\", actual_value.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx4-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
