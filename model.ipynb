{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Data Processing Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# SentencePiece\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model Creation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_sentence_generator(df):\n",
    "    for sentence in df:\n",
    "        yield sentence\n",
    "        \n",
    "def get_sentence_tokens(sentence):\n",
    "    '''Gets the word tokens of a given sentence by tokenising'''\n",
    "    tokens = sp.encode_as_pieces(sentence)\n",
    "    return tokens\n",
    "\n",
    "def get_avg_vector_for_sent(tokens, model):\n",
    "    '''Uses the Word2Vec LUT to get the vector representation of a tokens \n",
    "       in a sentence and takes the mean'''\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    for token in tokens:\n",
    "        if token in model.wv:\n",
    "            vector += model.wv[token]\n",
    "    if len(tokens) > 0: \n",
    "        vector /= len(tokens)\n",
    "    return vector\n",
    "    \n",
    "def sentence_to_vector(sentence, model):\n",
    "    '''Returns the vector representation of a sentence\n",
    "       by taking the average of the Word2Vec representations of the \n",
    "       words / subwords'''\n",
    "    tokens = get_sentence_tokens(sentence)\n",
    "    vector = get_avg_vector_for_sent(tokens, model)\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing to Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpvotesPredictorNN(nn.Module):\n",
    "    def __init__(self, word_dimensionality):\n",
    "        super(UpvotesPredictorNN, self).__init__()\n",
    "        \n",
    "        # Input layer to 1st hidden layer\n",
    "        self.fc1 = nn.Linear(word_dimensionality, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # 1st hidden layer to 2nd hidden layer\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # 2nd hidden layer to output layer\n",
    "        self.output = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        output = self.output(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if os.sys.platform == 'darwin':      # MacOS\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "elif os.sys.platform == 'win32':    # Windows\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:                               # other OS\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/TitlesAndScoreALL.csv'\n",
    "data_df = pd.read_csv(file_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Embedding Dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dimensionality = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: ../data/spm_model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 12828\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ‚Åá \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 165806 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=7517641\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9504% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=97\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999504\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 165803 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=3523094\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 168910 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 165803\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 133842\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 133842 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=64578 obj=12.503 num_tokens=286519 num_tokens/piece=4.43679\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=57396 obj=10.2535 num_tokens=288136 num_tokens/piece=5.02014\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=43035 obj=10.2693 num_tokens=303906 num_tokens/piece=7.06183\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=43005 obj=10.2433 num_tokens=304356 num_tokens/piece=7.07722\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=32253 obj=10.3802 num_tokens=326171 num_tokens/piece=10.1129\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=32253 obj=10.3468 num_tokens=326172 num_tokens/piece=10.1129\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=24189 obj=10.5375 num_tokens=350443 num_tokens/piece=14.4877\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=24189 obj=10.4962 num_tokens=350503 num_tokens/piece=14.4902\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=18141 obj=10.7321 num_tokens=376066 num_tokens/piece=20.7302\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=18141 obj=10.6849 num_tokens=376095 num_tokens/piece=20.7318\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=14110 obj=10.9301 num_tokens=398733 num_tokens/piece=28.2589\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=14110 obj=10.8832 num_tokens=398739 num_tokens/piece=28.2593\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: ../data/spm_model.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: ../data/spm_model.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    sentence_iterator=dataframe_sentence_generator(data_df['title']), \n",
    "    vocab_size=12_828,\n",
    "    model_prefix='../data/spm_model', \n",
    "    model_type='unigram',\n",
    ")\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('../data/spm_model.model')\n",
    "\n",
    "tokenized_titles = [sp.encode_as_pieces(title) for title in data_df['title']]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_titles, vector_size=word_dimensionality, window=5, min_count=1, workers=4)\n",
    "data_df['sentence_vector'] = data_df['title'].apply(lambda x: sentence_to_vector(x, word2vec_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test_and_val, y_train, y_test_and_val = train_test_split(\n",
    "    data_df['sentence_vector'].tolist(), \n",
    "    data_df['score'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test_and_val, \n",
    "    y_test_and_val, \n",
    "    test_size=0.5, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train = torch.tensor(np.vstack(X_train), dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(np.array(y_train), dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "X_val = torch.tensor(np.vstack(X_val), dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(np.array(y_val), dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "X_test = torch.tensor(np.vstack(X_test), dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(np.array(y_test), dtype=torch.float32).view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and hyperparameters\n",
    "model = UpvotesPredictorNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 2000\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Storage object for data vis\n",
    "list_of_lists = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train.to(device))\n",
    "    loss = criterion(outputs, y_train.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test.to(device))\n",
    "        val_loss = criterion(predictions, y_test.to(device))\n",
    "    \n",
    "    # Store loss for data vis\n",
    "    list_of_lists.append([epoch, loss.item(), val_loss.item()])\n",
    "    \n",
    "    # # Logging\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1},\\t Train Loss: {loss.item()},\\t Val Loss: {val_loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "epoch_list = []\n",
    "\n",
    "for i,j,k in list_of_lists:\n",
    "    epoch_list.append(i)\n",
    "    train_loss_list.append(j)\n",
    "    val_loss_list.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_list, train_loss_list, label='Train Loss')\n",
    "plt.plot(epoch_list, val_loss_list, label='Val Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predictions = model(X_test)\n",
    "    test_loss = criterion(predictions, y_test) \n",
    "\n",
    "print(f'Test Loss: {test_loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
