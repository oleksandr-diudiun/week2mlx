{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c57af6b-732b-4ffc-b0b3-fa7d4ad524df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataLoade' from 'torch.utils.data' (/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentencepiece\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspm\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoade\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DataLoade' from 'torch.utils.data' (/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/__init__.py)"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from gensim.models import Word2Vec\n",
    "from torch.utils.data import Dataset, DataLoade\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c78dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_sentence_generator(df):\n",
    "    for sentence in df:\n",
    "        yield sentence\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file\n",
    "file_path = 'Dataset1000WithScore.txt'\n",
    "df = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_texts, test_texts, train_score, test_score = train_test_split(\n",
    "    df['title'], \n",
    "    df['score'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7b960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: spm_Alex_week2\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2400\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 800 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=35374\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9519% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=83\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999519\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 800 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=15772\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 5929 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 800\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 3071\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 3071 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=2697 obj=14.1621 num_tokens=6926 num_tokens/piece=2.56804\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=2438 obj=12.7731 num_tokens=6984 num_tokens/piece=2.86464\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: spm_Alex_week2.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: spm_Alex_week2.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    sentence_iterator=dataframe_sentence_generator(train_texts), \n",
    "    model_prefix='spm_Alex_week2', \n",
    "    vocab_size=2400\n",
    ")\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('spm_Alex_week2.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1f3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Thi', 's', '▁', 'is', '▁a', '▁test', '...', '.', '?', '?', '?']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.encode_as_pieces('This is a test....???')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_titles = [sp.encode_as_pieces(title) for title in train_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441a9fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_titles, \n",
    "    vector_size=100, \n",
    "    window=5, \n",
    "    min_count=1, \n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3129d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_to_embedding(title):\n",
    "    tokens = sp.encode_as_pieces(title)\n",
    "    embeddings = [w2v_model.wv[token] for token in tokens if token in w2v_model.wv]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
